{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a125fda",
   "metadata": {},
   "source": [
    "# 작사가 인공지능 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02873ac6",
   "metadata": {},
   "source": [
    "## 데이터 다듬기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a25b14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터 크기: 187088\n",
      "Examples:\n",
      " [\"Now I've heard there was a secret chord\", 'That David played, and it pleased the Lord', \"But you don't really care for music, do you?\"]\n"
     ]
    }
   ],
   "source": [
    "import os, re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "txt_file_path = os.getenv('HOME')+'/aiffel/lyricist/data/lyrics/*'\n",
    "\n",
    "txt_list = glob.glob(txt_file_path)\n",
    "\n",
    "raw_corpus = []\n",
    "\n",
    "for txt_file in txt_list:\n",
    "    with open(txt_file, \"r\") as f:\n",
    "        raw = f.read().splitlines()\n",
    "        raw_corpus.extend(raw)\n",
    "\n",
    "print(\"데이터 크기:\", len(raw_corpus))\n",
    "print(\"Examples:\\n\", raw_corpus[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa1f0efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now I've heard there was a secret chord\n",
      "That David played, and it pleased the Lord\n",
      "But you don't really care for music, do you?\n",
      "It goes like this\n",
      "The fourth, the fifth\n",
      "The minor fall, the major lift\n",
      "The baffled king composing Hallelujah Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah\n",
      "Hallelujah Your faith was strong but you needed proof\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(raw_corpus):\n",
    "    if len(sentence) == 0: continue \n",
    "    if sentence[-1] == \":\": continue \n",
    "\n",
    "    if idx > 9: break  \n",
    "        \n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83f672a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> this is sample sentence . <end>\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip() # 1\n",
    "    sentence = re.sub(r\"([?.!,¿])\", r\" \\1 \", sentence) # 2\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence) # 3\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", sentence) # 4\n",
    "    sentence = sentence.strip() # 5\n",
    "    sentence = '<start> ' + sentence + ' <end>' # 6\n",
    "    return sentence\n",
    "\n",
    "print(preprocess_sentence(\"This @_is ;;;sample        sentence.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d9b2add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<start> now i ve heard there was a secret chord <end>',\n",
       " '<start> that david played , and it pleased the lord <end>',\n",
       " '<start> but you don t really care for music , do you ? <end>',\n",
       " '<start> it goes like this <end>',\n",
       " '<start> the fourth , the fifth <end>',\n",
       " '<start> the minor fall , the major lift <end>',\n",
       " '<start> the baffled king composing hallelujah hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah <end>',\n",
       " '<start> hallelujah your faith was strong but you needed proof <end>']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus = []\n",
    "\n",
    "for sentence in raw_corpus:\n",
    "    if len(sentence) == 0: continue\n",
    "    if sentence[-1] == \":\": continue\n",
    "    \n",
    "    preprocessed_sentence = preprocess_sentence(sentence)\n",
    "    corpus.append(preprocessed_sentence)\n",
    "        \n",
    "corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1b6574e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5 ...    0    0    0]\n",
      " [   2   17 2639 ...    0    0    0]\n",
      " [   2   36    7 ...   43    3    0]\n",
      " ...\n",
      " [   5   22    9 ...   10 1013    3]\n",
      " [  37   15 9049 ...  877  647    3]\n",
      " [   2    7   34 ...    0    0    0]] <keras_preprocessing.text.Tokenizer object at 0x7faf1c4ac100>\n"
     ]
    }
   ],
   "source": [
    "def tokenize(corpus):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "        num_words= 12000,\n",
    "        filters=' ',\n",
    "        oov_token=\"<unk>\"\n",
    "    )\n",
    "\n",
    "    tokenizer.fit_on_texts(corpus)\n",
    "    tensor = tokenizer.texts_to_sequences(corpus)   \n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)  \n",
    "    \n",
    "    print(tensor,tokenizer)\n",
    "    return tensor, tokenizer\n",
    "\n",
    "tensor, tokenizer = tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3046f7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   2   50    5   91  297   65   57    9  969 6042]\n",
      " [   2   17 2639  873    4    8   11 6043    6  329]\n",
      " [   2   36    7   37   15  164  282   28  299    4]]\n"
     ]
    }
   ],
   "source": [
    "print(tensor[:3, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c24ca400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : <unk>\n",
      "2 : <start>\n",
      "3 : <end>\n",
      "4 : ,\n",
      "5 : i\n",
      "6 : the\n",
      "7 : you\n",
      "8 : and\n",
      "9 : a\n",
      "10 : to\n"
     ]
    }
   ],
   "source": [
    "for idx in tokenizer.index_word:\n",
    "    print(idx, \":\", tokenizer.index_word[idx])\n",
    "\n",
    "    if idx >= 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ca0c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2   50    5   91  297   65   57    9  969 6042    3    0    0    0]\n",
      "[  50    5   91  297   65   57    9  969 6042    3    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "src_input = tensor[:, :-1]\n",
    "tgt_input = tensor[:, 1:]\n",
    "\n",
    "print(src_input[0])\n",
    "print(tgt_input[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d0c7cf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<BatchDataset shapes: ((256, 14), (256, 14)), types: (tf.int32, tf.int32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BUFFER_SIZE = len(src_input)\n",
    "BATCH_SIZE = 256\n",
    "steps_per_epoch = len(src_input) // BATCH_SIZE\n",
    "\n",
    "VOCAB_SIZE = tokenizer.num_words + 1   \n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((src_input, tgt_input))\n",
    "dataset = dataset.shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4053ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)\n",
    "        self.rnn_1 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.rnn_2 = tf.keras.layers.LSTM(hidden_size, return_sequences=True)\n",
    "        self.linear = tf.keras.layers.Dense(vocab_size)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.rnn_1(out)\n",
    "        out = self.rnn_2(out)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "embedding_size = 256\n",
    "hidden_size = 1024\n",
    "lyricist = TextGenerator(tokenizer.num_words + 1, embedding_size , hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7dd8ee35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "enc_train, enc_val, dec_train, dec_val = train_test_split(src_input, tgt_input, test_size=0.2, random_state=77)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f8c04",
   "metadata": {},
   "source": [
    "## 인공지능 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34c3c1d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(256, 14, 12001), dtype=float32, numpy=\n",
       "array([[[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [ 5.67520765e-05, -8.40629655e-05, -5.51901117e-04, ...,\n",
       "          4.26964776e-04, -7.20167227e-05,  4.48473613e-04],\n",
       "        [ 2.46885786e-04, -2.49523378e-04, -5.06411132e-04, ...,\n",
       "          4.43717552e-04, -3.75943579e-04,  6.63637766e-04],\n",
       "        ...,\n",
       "        [-6.91201421e-04,  1.00111403e-03,  1.02094002e-03, ...,\n",
       "          1.83974043e-03,  1.16257183e-03,  1.05904233e-04],\n",
       "        [-6.33073447e-04,  9.23938351e-04,  1.24200224e-03, ...,\n",
       "          1.76542182e-03,  1.13758212e-03, -1.23777412e-04],\n",
       "        [-3.96912685e-04,  6.79556921e-04,  1.36858283e-03, ...,\n",
       "          1.61755783e-03,  1.00647600e-03, -3.06890463e-04]],\n",
       "\n",
       "       [[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [-1.79549970e-04, -1.09416578e-04, -3.99316574e-04, ...,\n",
       "          2.87612405e-04,  2.07408331e-04,  5.61703870e-04],\n",
       "        [-3.86205764e-04,  4.87394718e-05, -3.28533293e-04, ...,\n",
       "          6.60085352e-04,  3.30298171e-05,  8.76360165e-04],\n",
       "        ...,\n",
       "        [ 8.36720690e-04, -1.25685008e-03,  9.55336494e-04, ...,\n",
       "         -1.71618725e-04, -2.99855583e-05,  1.39630749e-04],\n",
       "        [ 1.16970611e-03, -1.37371477e-03,  1.09707308e-03, ...,\n",
       "         -3.18677950e-04,  8.27731274e-05,  7.98120018e-05],\n",
       "        [ 1.46675459e-03, -1.45276403e-03,  1.23126910e-03, ...,\n",
       "         -4.65207791e-04,  2.30176738e-04,  3.87524233e-05]],\n",
       "\n",
       "       [[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [-2.50413286e-04,  1.13588416e-04, -4.76019166e-04, ...,\n",
       "          3.20650171e-04,  1.35926297e-04,  2.67270516e-04],\n",
       "        [-3.70521797e-04,  3.17408063e-04, -4.72797808e-04, ...,\n",
       "          5.27911761e-04,  2.16793953e-04,  2.36641208e-04],\n",
       "        ...,\n",
       "        [-2.29273355e-04,  1.30264042e-03, -6.23451488e-04, ...,\n",
       "          9.48780857e-04,  9.63292667e-04, -8.83637331e-05],\n",
       "        [-1.40871562e-04,  1.29165594e-03, -4.40981588e-04, ...,\n",
       "          1.26791210e-03,  4.56480077e-04, -1.68693514e-04],\n",
       "        [-1.59752293e-04,  1.29637355e-03, -1.92276420e-04, ...,\n",
       "          1.44623255e-03,  2.67341005e-04, -2.10914077e-04]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [-3.61097220e-04, -1.13073169e-04, -8.18991335e-04, ...,\n",
       "          6.59148267e-04,  3.09426432e-05,  3.99655022e-04],\n",
       "        [-6.30106602e-04,  8.81972737e-05, -1.02953229e-03, ...,\n",
       "          1.13796408e-03, -2.07403224e-04,  5.98354614e-04],\n",
       "        ...,\n",
       "        [ 5.89851232e-04, -2.18983056e-04, -5.97219972e-04, ...,\n",
       "          8.59198510e-04, -6.10929914e-04, -1.57373594e-04],\n",
       "        [ 9.78339813e-04, -4.37514565e-04, -2.48365512e-04, ...,\n",
       "          5.54568251e-04, -4.50611435e-04, -1.94711101e-04],\n",
       "        [ 1.32183288e-03, -6.20004721e-04,  8.48460040e-05, ...,\n",
       "          2.70794495e-04, -2.56725412e-04, -2.05111442e-04]],\n",
       "\n",
       "       [[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [-2.07702644e-04, -1.45865139e-04, -3.03026754e-04, ...,\n",
       "          7.39273033e-04, -1.16415686e-04,  2.15631590e-05],\n",
       "        [-6.83901817e-07, -2.16853558e-04, -3.31960095e-04, ...,\n",
       "          1.31667452e-03, -3.43514985e-04, -6.77013522e-05],\n",
       "        ...,\n",
       "        [-9.75346891e-04, -2.76169478e-04,  2.83104891e-04, ...,\n",
       "          1.75744365e-03, -1.19406299e-03, -4.98297682e-04],\n",
       "        [-1.12168479e-03, -1.02070699e-04,  4.30212734e-04, ...,\n",
       "          1.66128390e-03, -9.44096770e-04, -5.42626658e-04],\n",
       "        [-1.03928358e-03, -1.75791633e-04,  5.32203238e-04, ...,\n",
       "          1.51205831e-03, -8.12891056e-04, -6.34896976e-04]],\n",
       "\n",
       "       [[-1.55151894e-04, -7.37548471e-05, -3.34326382e-04, ...,\n",
       "          3.40311410e-04,  8.85685949e-05,  2.41720103e-04],\n",
       "        [-1.36948991e-04, -2.26386503e-04, -2.95034784e-04, ...,\n",
       "          6.88267057e-04,  1.25977705e-04,  4.88376827e-04],\n",
       "        [-1.21110247e-06, -3.37152247e-04, -3.28994240e-04, ...,\n",
       "          7.19692674e-04, -1.82480471e-05,  1.04077987e-03],\n",
       "        ...,\n",
       "        [ 8.52122437e-04, -6.76878612e-04,  5.10056037e-04, ...,\n",
       "          2.51508609e-04, -3.07025941e-04,  5.85893460e-04],\n",
       "        [ 1.19621400e-03, -8.94643657e-04,  6.70022157e-04, ...,\n",
       "         -6.37651010e-06, -2.01453498e-04,  3.99998244e-04],\n",
       "        [ 1.50557572e-03, -1.07268163e-03,  8.29970115e-04, ...,\n",
       "         -2.44840077e-04, -5.24811148e-05,  2.61640613e-04]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for src_sample, tgt_sample in dataset.take(1): break\n",
    "\n",
    "lyricist(src_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "621e4d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"text_generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        multiple                  3072256   \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  multiple                  5246976   \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                multiple                  8392704   \n",
      "_________________________________________________________________\n",
      "dense (Dense)                multiple                  12301025  \n",
      "=================================================================\n",
      "Total params: 29,012,961\n",
      "Trainable params: 29,012,961\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lyricist.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "23f127e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "686/686 [==============================] - 115s 165ms/step - loss: 3.6114\n",
      "Epoch 2/10\n",
      "686/686 [==============================] - 113s 164ms/step - loss: 3.1217\n",
      "Epoch 3/10\n",
      "686/686 [==============================] - 113s 164ms/step - loss: 2.9337\n",
      "Epoch 4/10\n",
      "686/686 [==============================] - 113s 164ms/step - loss: 2.7889\n",
      "Epoch 5/10\n",
      "686/686 [==============================] - 113s 164ms/step - loss: 2.6617\n",
      "Epoch 6/10\n",
      "686/686 [==============================] - 113s 164ms/step - loss: 2.5459\n",
      "Epoch 7/10\n",
      "686/686 [==============================] - 113s 165ms/step - loss: 2.4381\n",
      "Epoch 8/10\n",
      "686/686 [==============================] - 113s 165ms/step - loss: 2.3368\n",
      "Epoch 9/10\n",
      "686/686 [==============================] - 113s 165ms/step - loss: 2.2404\n",
      "Epoch 10/10\n",
      "686/686 [==============================] - 113s 165ms/step - loss: 2.1488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fae8a91aaf0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True,\n",
    "    reduction='none'\n",
    ")\n",
    "\n",
    "lyricist.compile(loss=loss, optimizer=optimizer)\n",
    "lyricist.fit(dataset, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3558b9cf",
   "metadata": {},
   "source": [
    "## 잘 만들어졌는지 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c0fac96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(lyricist, tokenizer, init_sentence=\"<start>\", max_len=20):\n",
    "    test_input = tokenizer.texts_to_sequences([init_sentence])\n",
    "    test_tensor = tf.convert_to_tensor(test_input, dtype=tf.int64)\n",
    "    end_token = tokenizer.word_index[\"<end>\"]\n",
    "\n",
    "    while True:\n",
    "    \n",
    "        predict = lyricist(test_tensor)\n",
    "        predict_word = tf.argmax(tf.nn.softmax(predict, axis=-1), axis=-1)[:, -1] \n",
    "        test_tensor = tf.concat([test_tensor, tf.expand_dims(predict_word, axis=0)], axis=-1)\n",
    "        if predict_word.numpy()[0] == end_token: break\n",
    "        if test_tensor.shape[1] >= max_len: break\n",
    "\n",
    "    generated = \"\"\n",
    "    for word_index in test_tensor[0].numpy():\n",
    "        generated += tokenizer.index_word[word_index] + \" \"\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a0b0a91d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<start> i love you so much <end> '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(lyricist, tokenizer, init_sentence=\"<start> i love\", max_len=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29897d",
   "metadata": {},
   "source": [
    "# 회고"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1da64be",
   "metadata": {},
   "source": [
    "\n",
    " 알파고가 한창 이슈였던 때인 2016년 쯤을 돌이켜 보면, 저는 인공지능 즉 AI의 등장으로 없어질 직업이라는 기사가 떠오릅니다. 제 기억으로는 순위권에 경리와 회계사 등이 있었는데, 이번 Exploration의 주제인 작사가도 그 중 하나였습니다.\n",
    " \n",
    " AI가 번역을 해낸다는 것은 '뭐 그럴 수 있지?'라 생각하지만, 그 당시 AI가 글을 쓴다는 것은 상상하기 어려웠습니다. 물론 챗봇을 기획해보며, 무엇보다도 AIFFEL의 일원으로서 AI가 작사를 할 수 있다는 것을 너무나 당연히 알고 있습니다. 제가 어떤 과정을 거쳤는지에 대해서 그리고 어떤 부분을 새롭게 배웠는지 회고해보고자 합니다.\n",
    " \n",
    " 제가 작곡한 글자는 \"i love you so much\"입니다. 어떻게 보면 그럴 듯합니다. 누구나 작사에 사용하는 문구겠지만, 누구나 쓰기에 AI가 인간의 공감을 얻을 수 있는 작사가이지 않나라고 생각합니다.\n",
    " \n",
    " 먼저, 이번 Exploration에서 개인적으로 중요하다고 생각한 부분은 데이터 전처리 과정에서 부호 및 특수문자와 대문자 및 소문자를 정규표현식으로 필터링하는 것이라 생각합니다. 왜냐하면, 가장 기본이니깐? 처음 알게 된 방법이라서 그렇습니다. 서두에 말씀드린 것처럼 작사는 안될 거라는 것을 된다고 알려줬기 때문입니다. 이때 특별히 어려움은 없었기에 다음으로 진행했습니다.\n",
    " \n",
    " 다음으로, 텍스트 생성모델이 안정적으로 학습되게 하기 위해서 'num_words= 12000'를 조절해 validation loss를 2.2이하로 맞췄습니다. 또한, 문장 길이를 15를 최대로 맞추려고 'tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post', maxlen = 15)'를 추가해주었습니다.\n",
    " \n",
    " going deeper에서 NL를 고민하고 있는 저에게 좋은 프로젝트였다고 생각합니다. 이를 계기로 조금 더 나은 선택을 할 수 있고, 아이디어를 떠올릴 수 있을 것 같습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
